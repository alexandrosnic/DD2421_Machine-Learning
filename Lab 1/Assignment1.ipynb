{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first import the libraries we are going to use. \n",
    "monkdata package consists the three MONK databases.\n",
    "dtree package contains the required algorithms for building a decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import monkdata as m\n",
    "import dtree as d\n",
    "import drawtree_qt5 as qt5\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assignment_0: Which of the three databases is the most difficult for a decision tree algorithm to learn?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assignment_1: Calculate the entropy of the TRAINING datasets.\n",
    "\n",
    "(Note: Entropy is being used in order to define which is the best \"question\" - attribute - to choose from, to split the input data into subsets. For that reason, we have to run the entropy algorithm on the training set, not the test set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.957117428264771\n",
      "0.9998061328047111\n"
     ]
    }
   ],
   "source": [
    "# Calculate the entropy of each monk dataset\n",
    "\n",
    "monk1_entropy = d.entropy(m.monk1)\n",
    "print(monk1_entropy)\n",
    "\n",
    "monk2_entropy = d.entropy(m.monk2)\n",
    "print(monk2_entropy)\n",
    "\n",
    "monk3_entropy = d.entropy(m.monk3)\n",
    "print(monk3_entropy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assignment_2a: Explain entropy for a uniform distribution and a non-uniform distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assignment_2b: Present some example distributions with high and low entropy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assignment_3: Calculate the expected information gain corresponding to each of the six attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.07527255560831925, 0.005838429962909286, 0.00470756661729721, 0.02631169650768228, 0.28703074971578435, 0.0007578557158638421], [0.0037561773775118823, 0.0024584986660830532, 0.0010561477158920196, 0.015664247292643818, 0.01727717693791797, 0.006247622236881467], [0.007120868396071844, 0.29373617350838865, 0.0008311140445336207, 0.002891817288654397, 0.25591172461972755, 0.007077026074097326]]\n"
     ]
    }
   ],
   "source": [
    "infGain_monk1 = []\n",
    "infGain_monk2 = []\n",
    "infGain_monk3 = []\n",
    "\n",
    "# Calculate the information gain of each attribute, for each monk dataset\n",
    "for i in range(len(m.attributes)):\n",
    "    infGain_monk1.append(d.averageGain(m.monk1, m.attributes[i]))\n",
    "    infGain_monk2.append(d.averageGain(m.monk2, m.attributes[i]))\n",
    "    infGain_monk3.append(d.averageGain(m.monk3, m.attributes[i]))\n",
    "infGainMatrix = [infGain_monk1, \n",
    "                 infGain_monk2,\n",
    "                 infGain_monk3]\n",
    "print(infGainMatrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it seems from the information gain matrix, the 5th attribute is the most effective to be used for splitting the 1st and 2nd databse, whereas for the 3rd database should be the 2nd attribute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assignment_4: How does the entropy of the subsets, Sk, look like when the information gain is maximized?\n",
    "How can we motivate using the information gain as a heuristic for picking an attribute for splitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Information gain is maximized either when the subset Sk formed using the k attribute, is much smaller than the initial S subset, either when the Entropy of using the k attribute is minimalized. That happens when, using the k attribute, the predictability of the result of the dataset increases. In other words, which of the attributes will give us the most clear distinction between the subsets, if we use it to split the initial dataset?\n",
    "\n",
    "=> Entropy increases => Unpredictable result => Attribute unsuitable for splitting\n",
    "Entropy decreases => Predictable result => Attribute suitable for splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.07527255560831925, 0.005838429962909286, 0.00470756661729721, 0.02631169650768228, 0.28703074971578435, 0.0007578557158638421, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "A5(+A4(---)A6(--)A1(--+))\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 0\n"
     ]
    }
   ],
   "source": [
    "# Split the monk1 dataset to more subsets, for when the 5th attribute has the x value. What is the value??\n",
    "monk1_level1 = d.select(m.monk1, m.attributes[4], 1)\n",
    "\n",
    "# Calculate again the information gain for the new level\n",
    "for i in range(len(m.attributes)):\n",
    "    infGain_monk1.append(d.averageGain(monk1_level1, m.attributes[i]))\n",
    "    \n",
    "print(infGain_monk1)\n",
    "\n",
    "# We have to do the same for level 2\n",
    "\n",
    "# Then obtain the majority class of each new subset, after the split\n",
    "d.mostCommon(monk1_level1)\n",
    "\n",
    "# Compare it with the predefined way to build a tree\n",
    "# How compare it?\n",
    "\n",
    "# Automatically build tree for 1st dataset\n",
    "buildedTree1 = d.buildTree(m.monk1, m.attributes,2)\n",
    "print(buildedTree1)\n",
    "\n",
    "# Visualize the builded tree\n",
    "qt5.drawTree(buildedTree1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assignment_5a: Build the full decision trees for all three Monk datasets using buildTree. Then, use the function check to measure the performance of the decision tree on both the training and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatically build trees for all the datasets\n",
    "\n",
    "buildedTree2 = d.buildTree(m.monk2, m.attributes,2)\n",
    "buildedTree3 = d.buildTree(m.monk3, m.attributes,2)\n",
    "\n",
    "# Measure the performance of the decision trees on both the training and test datasets.\n",
    "# d.check returns the number of the test samples that are positive when splitted using the proposed decision tree\n",
    "print(d.check(buildedTree1, m.monk1test)) \n",
    "print(d.check(buildedTree2, m.monk2test))\n",
    "print(d.check(buildedTree3, m.monk3test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assignment_5b: Compute the train and test set errors for the three Monk datasets for the full trees. Were your assumptions about the datasets correct? Explain the results you get for the training and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the error => Does it means the misclassification rate? I think it is actually the d.check command \n",
    "# that we did before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reduced error pruning.\n",
    "We don't prune over test set because then we won't have a set to validate the results.\n",
    "Then we randomly partition our training set into training and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition(data, fraction):\n",
    "    ldata = list(data)\n",
    "    random.shuffle(ldata)\n",
    "    breakPoint = int(len(ldata) * fraction)\n",
    "    return ldata[:breakPoint], ldata[breakPoint:]\n",
    "\n",
    "monk1train, monk1val = partition(m.monk1, 0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write code which performs the complete pruning by repeatedly calling allPruned and picking the tree which gives the best classification performance on the validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'branches'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-50-da0a70dbe5b7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mPruned1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mallPruned\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuildedTree1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPruned1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmonk1val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Masters\\Machine Learning\\Assignment 1\\dtree.py\u001b[0m in \u001b[0;36mcheck\u001b[1;34m(tree, testdata)\u001b[0m\n\u001b[0;32m    117\u001b[0m     \u001b[0mcorrect\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtestdata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mclassify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpositive\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    120\u001b[0m             \u001b[0mcorrect\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorrect\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Masters\\Machine Learning\\Assignment 1\\dtree.py\u001b[0m in \u001b[0;36mclassify\u001b[1;34m(tree, sample)\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTreeLeaf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 112\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mclassify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbranches\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattribute\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattribute\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    113\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'branches'"
     ]
    }
   ],
   "source": [
    "Pruned1 = d.allPruned(buildedTree1)\n",
    "print(d.check(Pruned1, monk1val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
